setwd("c:/stata")
setwd("c:/stata")
library(haven)
hprice2 <- read_dta(file="hprice2.dta")
reg1 <- lm(log(price)~ rooms + I(rooms^2) , data=hprice2)
summary(reg1)
#############################
setwd("c:/stata")
usedcars <- read.csv(file="usedcars.csv", header=T, stringsAsFactors = F)
str(usedcars)
setwd("c:/stata") #
library(haven)
vote1 <- read_dta(file="vote1.dta")
reg1 <- lm(voteA ~ shareA , data=vote1)
summary(reg1)
plot(voteA~shareA, data=vote1)  # scatter plot
abline(reg1)
ceosal1 <- read_dta(file="ceosal1.dta")
reg2 <- lm(salary ~ roe, data=ceosal1)
summary(reg2)
uhat <- reg2$residuals
uhat
ceosal1$uhat <- reg2$residuals   #변수 추가하고싶다 !
ceosal1$yhat <- reg2$fitted.values
beta_hat <- reg2$coefficients  # coefficient(reg2)  # 밑에랑 같은 말 ㅇㅇ
beta_hat1 <- coefficients(reg2)  # coef(reg2) = coefficients(reg2)
beta_hat1
summary(reg2)
setwd("c:/stata")
library(haven)
gpa1 <- read_dta(file="gpa1.dta")
# y: colGPA, x1: hsGPA, x2: ACT
# y=beta0+beta1*x1+beta2*x2 + u
reg1 <- lm(colGPA ~ hsGPA + ACT, data=gpa1)
summary(reg1)
# partialing out interpretation
# y: hsGPA, x: ACT
reg2 <-lm(hsGPA ~ ACT, data=gpa1)
str(reg2)  # residuals : r_hat1
gpa1$resid <- reg2$residuals
reg3 <- lm(colGPA ~ resid, data=gpa1)  #위랑 똑같은 값나옴 !!
summary(reg3)
reg1 <- lm(colGPA ~ hsGPA + ACT, data=gpa1)
summary(reg1)
str(reg1)
setwd("c:/stata")
reg4 <- lm(log(wage)~ educ + exper , data=wage1)
sum4 <- summary(reg4)
str(sum4)
str(reg4)
sum4$sigma               #s.e 보고싶다면 summary에서 볼 수 있다.
reg4$sigma
setwd("c:/stata")
library(haven)
setwd("c:/stata")
library(haven)
aa <- read_dta(file="mid_1.dta")
View(aa)
str(aa)
quantile(aa$price, prob=seq(from=0, to=1, by=0.1))
bb <- read_dta(file="mid_2.dta")
View(bb)
var(bb$x)*100
summary(bb)
var(bb)
var(bb$x)
setwd("c:/stata")
library(haven)
aa <- read_dta(file="mid_3.dta")
View(aa)
reg4 <- lm(pizza~ log(income) + age , data=aa)
sum4 <- summary(reg4)
str(sum4)
reg4
summary(reg4)
vcov(reg4)
1-pt(2, 20)
0.02963277*2
setwd("c:/stata")
library(haven)
aa <- read_dta(file="mid_4.dta")
View(aa)
reg_ur <- lm(price ~ crime + rooms + dist, data=aa)
summary(reg_ur)
reg_r <- lm(price ~ crime + I(rooms + dist), data=aa)
summary(reg_r)
{(0.5431-0.2401)/2}/ {(1-0.5431)/503}
bwght <- read_dta(file="bwght.dta")
names(bwght)
reg4 <- lm(bwght ~ cigs , data=bwght)
summary(reg4)
reg5 <- lm(I(10*bwght)~ cigs, data=bwght )
summary(reg5)
(-30.1132)/(22.511510)
library(haven)
aa <- read_dta(file="mid_4.dta")
aa
summary(aa)
View(aa)
reg1 <- lm(log(price)~ rooms + I(rooms^2) , data=aa)
summary(reg1)
library(haven)
hprice2 <- read_dta(file="hprice2.dta")
reg4 <- lm(log(price)~ rooms + stratio, data=hprice2)
summary(reg4)
summary(reg1)
exp(3.01459-0.35561*6.5+0.05636*6.5+0.3129^2/2)
setwd("c:/stata")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
library(rpart)
library(rpart.plot)
library(rattle)
titanic <- read.csv(file="Titanic.csv", stringsAsFactors = F)   # survive가 y변수
titanic$Survived <- as.factor(titanic$Survived)
DT <- rpart(Survived ~ Sex+PClass , data=titanic, method="class")
summary(DT)
rpart.plot(DT)
table(titanic$Survived)
450/1313
fancyRpartPlot(DT)
##############################
# 학부계량경제학
# Lecture note 7 and 8
# 2020-11-16
#############################
setwd("c:/stata")
library(haven)
gpa3 <-read_dta(file="gpa3.dta")
# intercept dummy : d=female
str(gpa3$female)
gpa3$female <-as.factor(gpa3$female)
str(gpa3$female)
reg1 <-lm(cumgpa ~ female+sat+hsperc+tothrs, data=gpa3
,subset=(spring==1))
summary(reg1)
# slope dummy and intercept dummy
reg2 <-lm(cumgpa ~ female+sat+sat:female+hsperc+tothrs, data=gpa3
,subset=(spring==1))
summary(reg2)
reg3 <-lm(cumgpa ~ female*sat+hsperc+tothrs, data=gpa3
,subset=(spring==1))
summary(reg3)
reg4 <-lm(cumgpa ~ female*(sat+hsperc+tothrs), data=gpa3
,subset=(spring==1))
summary(reg4)
library(ggiraph)
library(ggiraphExtra)
library(ggplot2)
library(plyr)
reg5 <-lm(cumgpa ~ female*sat,data=gpa3 )
ggPredict(reg5, se=F, interactive=T)
# Python vs. R
library(car)
reg6 <-lm(cumgpa~ female+sat+female:sat, data=gpa3)
summary(reg6)
# H0: female1 coef=0 and female1:sat coef=0
myH0 <- c("female1","female1:sat")
linearHypothesis(reg6, myH0)
reg7 <-lm(cumgpa~ female*(sat+hsperc+tothrs), data=gpa3)
summary(reg7)
# H0: female1 coef=0 and female1:sat coef=0
myH0 <- c("female1","female1:sat"
,"female1:hsperc","female1:tothrs")
linearHypothesis(reg7, myH0)
# Chow test
# SSR_R, SSR_female, SSR_male
# 1) SSR_R
reg8 <-lm(cumgpa ~ sat + hsperc + tothrs, data=gpa3)
summary(reg8)
str(summary(reg8))
SSR_R <- 0.8671^2*728
SSR_R
# 2) SSR_female
reg9 <-lm(cumgpa ~ sat + hsperc + tothrs, data=gpa3,
subset=(female==1))
summary(reg9)
SSR_F <- 0.9036^2*176
SSR_F
# 3) SSR_male
reg10 <-lm(cumgpa ~ sat + hsperc + tothrs, data=gpa3,
subset=(female==0))
summary(reg10)
SSR_M <- 0.8443^2*548
SSR_M
# Chow test : F statistic
F1<- (SSR_R - (SSR_F+SSR_M))/ 4
F2<- (SSR_R)/(732-2*4)
F <- F1/F2
F
# subset=(x>900)
# subset=(x>900 & female==1)
setwd("c:/stata")
setwd("c:/stata")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
install.packages("rpart")
library(rpart)
library(rpart.plot)
library(rattle)
titanic <- read.csv(file="Titanic.csv", stringsAsFactors = F)
titanic$Survived <- as.factor(titanic$Survived)
DT <- rpart(Survived ~ Sex+PClass , data=titanic, method="class")
summary(DT)
rpart.plot(DT)
table(titanic$Survived)
450/1313
fancyRpartPlot(DT)
table(titanic$Survived)
rpart.plot(DT)
fancyRpartPlot(DT)
-0.6*log2(0.6)-0.4*log2(0.4)
curve(-x*log2(x)-(1-x)*log2(1-x), from=0, to=1, col="red")
credit <- read.csv(file="credit.csv",stringsAsFactors = F)
class(credit$default)
credit$default<-as.factor(credit$default)
class(credit$default)
prop.table(table(credit$default)) #root node
table(credit$default)
-0.7*log2(0.7)-0.3*log2(0.3)
set.seed(1234)
bs <- sample(1:1000, size=900, replace=F)
train <- credit[bs, ]
test <- credit[-bs, ]
tree1 <-rpart(default ~ age + housing, data=train, method="class")
rpart.plot(tree1)
install.packages("C50")
library(C50)
tree2 <- C5.0(x=train[,-17], y = train[,17])
tree2
summary(tree2)
setwd("c:/stata")
credit <-read.csv(file="credit.csv", stringsAsFactors = F)
library(C50)
credit_model <- C5.0(default ~ ., data=credit ) # .은 y변수를 제외한 나머지 모든 변수를 x변수로 하라는 의미
credit_model
credit$default<-as.factor(credit$default)
credit_model <- C5.0(default ~ ., data=credit ) # .은 y변수를 제외한 나머지 모든 변수를 x변수로 하라는 의미
credit_model
summary(credit_model)
credit_model1 <- C5.0(default ~ ., data=credit,
control=C5.0Control(minCases = 20))
plot(credit_model1)
install.packages("MASS") # 이 안에 예제데이터가 있음
library(MASS)
data(Boston)
force(Boston)
View(Boston)
install.packages("tree")
library(tree)
str(Boston$medv) #tree 란 놈은 y변수가 factor variable이면 classification으로 numeric이면 regression으로 자동으로 넘어감
single <-tree(medv ~ . , data=Boston)
single
summary(single) # 해석 2) 부분 rmdl 6.941보다 작은게 430개 있고 예측값은 19.93, 잔차의 제곱합이 17320
# tree construction 뭐시기란? 사용가능한 variables 중에 y변수 한개 뺀 것 중에 실제로 쓰인것 여기선 5개 !
#residual mean deviance = 6734는 전체 잔차제곱합 합친 거 , 전체 데이터 506개에서 tree size 9개를 빼줌. 여기서 tree size를 왜 빼주냐? 그래야 tree size가 클수록 deviance가 크게 나오기 때문 일종의 penalty
plot(single)
text(single, pretty=0)
setwd("c:/stata")
credit <-read.csv(file="credit.csv", stringsAsFactors = F)
str(credit$default)
credit$default<-as.factor(credit$default)
library(C50)
credit_model <- C5.0(default ~ ., data=credit ) # .은 y변수를 제외한 나머지 모든 변수를 x변수로 하라는 의미
credit_model      #tree size = terminal node의 수 에욤
summary(credit_model)
plot(credit_model)
credit_model1 <- C5.0(default ~ ., data=credit,
control=C5.0Control(minCases = 20)) # terminal node에 들어가는 표본의 수가 최소 20개는 되게해라
plot(credit_model1)
train <- credit[1:700,]
test <- credit[701:nrow(credit),]
credit_model1 <-C5.0(default~., data=train)
summary(credit_model1)
pred1 <- predict(credit_model1, newdata=test)
pred1
table(test$default, pred1)
credit_model3 <-C5.0(default~., data=train, trials=10) #trials = 10 은 열번 만들어라 ! 란 의미 tree를 열개 만들어라 !
summary(credit_model3)
pred3 <- predict(credit_model3, newdata=test)
pred3
table(test$default, pred3)
plot(credit_model3, trials=5)
install.packages("MASS") # 이 안에 예제데이터가 있음
library(MASS)
data(Boston)
install.packages("tree")  # tree는  classification이랑 regression 에서도 가능
library(tree)
str(Boston$medv)
single <-tree(medv ~ . , data=Boston)
single
summary(single)
plot(single)
text(single, pretty=0)
View(Boston)
train <- Boston(1:400,)
test <- Boston(401~nrow(Boston),)
Boston_model <- tree(medv ~ . , data = train)
str(Boston)
train <- data(Boston(1:400,))
View(credit)
str(credit)
train <- Boston(1:400,)
train <- Boston[1:400,]
test <- Boston[401~nrow(Boston),]
Boston_model <- tree(medv ~ . , data = train)
test <- Boston[401:nrow(Boston), ]
summary(Boston_model)
pred2 <- predict(Boston_model, newdata = test)
summary(pred2)
pred2
table(test$medv, pred2)
pred1
summary(Boston_model)
table(test$medv, pred2)
pred2
pred1
summary(pred2)
# Bootstrapping
set.seed(1234)
sample(1:10,replace=T )  #replace = T 복원추출
sample(1:10,size=5, replace=T )
# Bagging
install.packages("ipred")
library(ipred)
set.seed(300)
credit <-read.csv(file="credit.csv",
stringsAsFactors = F)
credit$default <- as.factor(credit$default)
install.packages("randomForest")
library(randomForest)
set.seed(1234)
rf <- randomForest(default ~ ., data=credit)
rf
rf <- randomForest(default ~ ., data=credit)
rf
setwd("c:/stata")
install.packages("kernlab")
setwd("c:/stata")
library(haven)
psysed <- read_dta(file="psysed.dta")
xx<-psysed[,2:3]
View(psysed)
set.seed(1234)
k4_fit <-kmeans(xx, center=4) # 4개의 중심점을 써라 . kmeans는 library 설치할 필요 X
k4_fit
install.packages("cluster")
setwd("c:/stata")
library(MASS)
data(Boston)
library(tree)
str(Boston$medv)
str(Boston$medv)
single <-tree(medv ~ . , data=Boston)
single
summary(single)
plot(single)
text(single, pretty=0)
train <- Boston[1:400,]
test <- Boston[401:nrow(Boston), ]
Boston_model <- tree(medv ~ . , data = train)
summary(Boston_model)
train <- Boston[1:400,]
test <- Boston[401:nrow(Boston), ]
Boston_model <- tree(medv ~ . , data = train)
summary(Boston_model)
pred2 <- predict(Boston_model, newdata = test)
pred2
table(test$medv, pred2)
prop.table(table(test$medv,pred2))
setwd("c:/stata")
set.seed(1234)
sample(1:10,replace=T )
sample(1:10,size=5, replace=T )
install.packages("ipred")
library(ipred)
set.seed(300)
credit <-read.csv(file="credit.csv",
stringsAsFactors = F)
credit$default <- as.factor(credit$default)
mybag <- bagging(default ~ . ,data=credit, nbagg=25 )
mybag
summary(mybag)
credit_pred <- predict(mybag, newdata=credit)
table(credit_pred)
table(credit$default, credit_pred)
train <- credit[1:900,]
test <- credit[901:nrow(credit),]
mybag <- bagging(default ~ . ,data=train, nbagg=25 )
credit_pred <- predict(mybag, newdata=test)
table(credit_pred)
table(test$default, credit_pred)
(59+15)/100 #이게 바로 ccr
install.packages("randomForest")
library(randomForest)
set.seed(1234)
rf <- randomForest(default ~ ., data=credit)
rf
View(credit)
train <- credit[1:900,]
test <- credit[901:nrow(credit),]
set.seed(1234)
rf <- randomForest(default ~ ., data=train)
rf_pred <-predict(rf, newdata=test, type="response")
rf_pred
table(test$default, rf_pred)
rf_pred <-predict(rf, newdata=test, type="prob")
rf_pred
rf <- randomForest(default ~ ., data=train, ntree=100)
rf
rf <- randomForest(default ~ ., data=train)
plot(rf)
rf
varImpPlot(rf)
varImpPlot(rf, n=5)
